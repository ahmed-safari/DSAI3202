{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Parallelism with Machine Learning: The Housing Prices Competition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the competition\n",
    "\n",
    "- The Housing Prices Competition train_dataset consists of various features of residential homes in Ames, Iowa, including both quantitative and categorical variables like the size of the property, the number of rooms, year built, and neighborhood quality.\n",
    "- It includes a set of 79 explanatory variables describing almost every aspect of the houses, allowing for in-depth analysis.\n",
    "- *The primary goal* of the competition is to predict **the final price of each home**, in this lab we will use *RandomForests*.\n",
    "- The models are evaluated on Root Mean Squared Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price, encouraging precise predictions over a range of housing prices.\n",
    "\n",
    "### File descriptions\n",
    "- *train.csv*: the training set used to train the model.\n",
    "- *test.csv*: the test set used to compute the performance of the model.\n",
    "- *train_data_description.txt*: full description of each column.\n",
    "### Useful train_data fields\n",
    "\n",
    "Here's a brief version of what you'll find in the train_data description file.\n",
    "\n",
    "- *SalePrice*: the property's sale price in dollars. This is the target variable that you're trying to predict.\n",
    "- *MSSubClass*: The building class\n",
    "- *MSZoning*: The general zoning classification\n",
    "\n",
    "Teh train_dataset is acessible here: https://www.kaggle.com/code/dansbecker/random-forests/tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and prepare the train_data\n",
    "*If you're curious about this the professor can explain it for you*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    MSSubClass  MSZoning  LotFrontage  LotArea  Street  LotShape  LandContour  \\\n",
      "Id                                                                              \n",
      "1           60         3         65.0     8450       1         3            3   \n",
      "2           20         3         80.0     9600       1         3            3   \n",
      "3           60         3         68.0    11250       1         0            3   \n",
      "4           70         3         60.0     9550       1         0            3   \n",
      "5           60         3         84.0    14260       1         0            3   \n",
      "\n",
      "    Utilities  LotConfig  LandSlope  ...  GarageQual  GarageCond  PavedDrive  \\\n",
      "Id                                   ...                                       \n",
      "1           0          4          0  ...           4           4           2   \n",
      "2           0          2          0  ...           4           4           2   \n",
      "3           0          4          0  ...           4           4           2   \n",
      "4           0          0          0  ...           4           4           2   \n",
      "5           0          2          0  ...           4           4           2   \n",
      "\n",
      "    WoodDeckSF  OpenPorchSF  EnclosedPorch  3SsnPorch  ScreenPorch  PoolArea  \\\n",
      "Id                                                                             \n",
      "1            0           61              0          0            0         0   \n",
      "2          298            0              0          0            0         0   \n",
      "3            0           42              0          0            0         0   \n",
      "4            0           35            272          0            0         0   \n",
      "5          192           84              0          0            0         0   \n",
      "\n",
      "    MiscVal  \n",
      "Id           \n",
      "1         0  \n",
      "2         0  \n",
      "3         0  \n",
      "4         0  \n",
      "5         0  \n",
      "\n",
      "[5 rows x 70 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the train_dataset\n",
    "file_path = 'data/housing_prices_data/train.csv'\n",
    "train_data = pd.read_csv(file_path, index_col=\"Id\")\n",
    "\n",
    "# Columns to be deleted\n",
    "columns_to_delete = ['MoSold', 'YrSold', 'SaleType', 'SaleCondition', 'Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature']\n",
    "\n",
    "# Delete the specified columns\n",
    "train_data_cleaned = train_data.drop(columns=columns_to_delete, axis=1)\n",
    "\n",
    "# Define the input features (X) and the output (y)\n",
    "X = train_data_cleaned.drop('SalePrice', axis=1)\n",
    "y = train_data_cleaned['SalePrice']\n",
    "\n",
    "# Identify the categorical columns in X\n",
    "categorical_columns = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Initialize a LabelEncoder for each categorical column\n",
    "label_encoders = {column: LabelEncoder() for column in categorical_columns}\n",
    "\n",
    "# Apply Label Encoding to each categorical column\n",
    "for column in categorical_columns:\n",
    "    X[column] = label_encoders[column].fit_transform(X[column])\n",
    "\n",
    "# Display the first few rows of X to confirm the encoding\n",
    "print(X.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1022, 70), (438, 70), (1022,), (438,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the first dataset (X, y) into train and test sets with a 70% - 30% split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "# Fill NaN values in X_train and X_val with the median of the respective columns\n",
    "X_train_filled = X_train.fillna(X_train.median())\n",
    "X_val_filled = X_val.fillna(X_val.median())\n",
    "\n",
    "(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First RandomForest Model\n",
    "This is the code for a simple trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on the validation data: 26057.941851126383\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Create a Random Forest model\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "rf_model.fit(X_train_filled, y_train)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "y_val_pred_filled = rf_model.predict(X_val_filled)\n",
    "\n",
    "# Calculate the RMSE on the validation data\n",
    "rmse_filled = sqrt(mean_squared_error(y_val, y_val_pred_filled))\n",
    "\n",
    "# Print the RMSE\n",
    "print(f'RMSE on the validation data: {rmse_filled}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of Random Forest Model\n",
    "The three most important parameters that typically have the most impact on the performance of a Random Forest model are:\n",
    "\n",
    "- *n_estimators*: This parameter specifies the number of trees in the forest. Generally, a higher number of trees increases the performance and makes the predictions more stable, but it also makes the computation slower. Selecting the right number of trees requires balancing between performance and computational efficiency.\n",
    "\n",
    "- *max_features*: This parameter defines the maximum number of features that are allowed to try in an individual tree. There are several options available for this parameter:\n",
    "\n",
    "    - *sqrt*: This is commonly used and means that the maximum number of features used at each split is the square root of the total number of features.\n",
    "    - *log2*: This is another typical option, meaning the log base 2 of the feature count is used.\n",
    "    - *A specific integer or float*: You can specify an exact number or a proportion of the total.\n",
    "\n",
    "- *max_depth*: This parameter specifies the maximum depth of each tree. Deeper trees can model more complex patterns, but they also risk overfitting. Limiting the depth of trees can improve the model's generalization and reduce overfitting. It's often useful to set this parameter to a finite value, especially when dealing with a large number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the best parameters sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters {'n_estimators': 100, 'max_features': None, 'max_depth': None} for RMSE = 26057.941851126383, MAPE: 9.83203095544113%\n",
      "The sequential execution time is 59.58460569381714\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Define the parameter ranges\n",
    "n_estimators_range = [10, 25, 50, 100, 200, 300, 400]\n",
    "max_features_range = ['sqrt', 'log2', None]  # None means using all features\n",
    "max_depth_range = [1, 2, 5, 10, 20, None]  # None means no limit\n",
    "\n",
    "# Initialize variables to store the best model and its RMSE and parameters\n",
    "best_rmse = float('inf')\n",
    "best_mape = float('inf')\n",
    "best_model = None\n",
    "best_parameters = {}\n",
    "\n",
    "# Loop over all possible combinations of parameters\n",
    "for n_estimators in n_estimators_range:\n",
    "    for max_features in max_features_range:\n",
    "        for max_depth in max_depth_range:\n",
    "            # Create and train the Random Forest model\n",
    "            rf_model = RandomForestRegressor(\n",
    "                n_estimators=n_estimators,\n",
    "                max_features=max_features,\n",
    "                max_depth=max_depth,\n",
    "                random_state=42\n",
    "            )\n",
    "            rf_model.fit(X_train_filled, y_train)\n",
    "            \n",
    "            # Make predictions and compute RMSE\n",
    "            y_val_pred = rf_model.predict(X_val_filled)\n",
    "            rmse = sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "            # Compute MAPE\n",
    "            mape = mean_absolute_percentage_error(y_val, y_val_pred) * 100\n",
    "            # print(f\"The parameters: {n_estimators}, {max_features}, {max_depth}. RMSE: {rmse}, MAPE: {mape}%\")\n",
    "            # If the model is better than the current best, update the best model and its parameters\n",
    "            if rmse < best_rmse:\n",
    "                best_rmse = rmse\n",
    "                best_mape = mape\n",
    "                best_model = rf_model\n",
    "                best_parameters = {\n",
    "                    'n_estimators': n_estimators,\n",
    "                    'max_features': max_features,\n",
    "                    'max_depth': max_depth\n",
    "                }\n",
    "print(f\"The best parameters {best_parameters} for RMSE = {best_rmse}, MAPE: {mape}%\")\n",
    "end_time = time.time()\n",
    "sequential_time = start_time - end_time\n",
    "print(f\"The sequential execution time is {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the process of sequentially finding the best parameters for the Random Forest Model is slow and inefficient. We can use parallelism to speed up this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.d. Parallelize with Threading and processes\n",
    "\n",
    "In this section, we will try to parallelize the process of finding the best parameters for the Random Forest Model. We will use threading, and multiprocessing. In the end, we will compare the performance of the two methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelizing with Threading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by importing the necessary libraries\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to store the best model and its parameters\n",
    "best_rmse = float('inf')\n",
    "best_mape = float('inf')\n",
    "best_model = None\n",
    "best_parameters = {}\n",
    "lock = threading.Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the test and evaluation into a function\n",
    "\n",
    "def evaluate_model(n_estimators, max_features, max_depth):\n",
    "    global best_rmse, best_mape, best_model, best_parameters\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_features=max_features,\n",
    "        max_depth=max_depth,\n",
    "        random_state=42\n",
    "    )\n",
    "    rf_model.fit(X_train_filled, y_train)\n",
    "    y_val_pred = rf_model.predict(X_val_filled)\n",
    "    rmse = sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "    mape = mean_absolute_percentage_error(y_val, y_val_pred) * 100\n",
    "    \n",
    "    with lock: # Lock the critical section\n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_mape = mape\n",
    "            best_model = rf_model\n",
    "            best_parameters = {\n",
    "                'n_estimators': n_estimators,\n",
    "                'max_features': max_features,\n",
    "                'max_depth': max_depth\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters {'n_estimators': 100, 'max_features': None, 'max_depth': None} for RMSE = 26057.941851126383, MAPE: 9.83203095544113%\n",
      "The threaded execution time is 35.15937113761902\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "threads = []\n",
    "\n",
    "for n_estimators in n_estimators_range:\n",
    "    for max_features in max_features_range:\n",
    "        for max_depth in max_depth_range:\n",
    "            thread = threading.Thread(target=evaluate_model, args=(n_estimators, max_features, max_depth))\n",
    "            threads.append(thread)\n",
    "            thread.start()\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "end_time = time.time()\n",
    "threaded_time = end_time - start_time\n",
    "print(f\"The best parameters {best_parameters} for RMSE = {best_rmse}, MAPE: {mape}%\")\n",
    "print(f\"The threaded execution time is {threaded_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelizing with Multiprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will be using multiprocessing to parallelize the process of finding the best parameters for the Random Forest Model. We will use the same approach as in the previous section, but instead of using threading, we will use the multiprocessing module to create separate processes for each model evaluation.\n",
    "\n",
    "For each combination of hyperparameters, a separate process is spawned.\n",
    "Each process runs the evaluate_model function independently. This function Trains a RandomForestRegressor model with a specific set of hyperparameters.\n",
    "Evaluates the model by predicting on a validation set and calculating the RMSE and MAPE.\n",
    "Stores the hyperparameters and corresponding evaluation metrics (RMSE and MAPE) in a JSON file, with one file per hyperparameter combination.\n",
    "\n",
    "\n",
    "- Synchronization and Result Collection:\n",
    "\n",
    "After starting all processes, the main process waits for them to complete their execution using the join() method. This ensures that all model evaluations are finished before proceeding to the result collection phase.\n",
    "Once all processes have completed, the main process reads the JSON files, each containing the results of a single model evaluation.\n",
    "\n",
    "\n",
    "- Identifying the Best Model:\n",
    "\n",
    "The main process compares the RMSE from each model's evaluation results. It keeps track of the best (lowest) RMSE and corresponding MAPE and hyperparameters.\n",
    "After iterating through all files, it identifies the best model's hyperparameters and its performance metrics.\n",
    "\n",
    "- Cleanup and Final Output:\n",
    "\n",
    "Temporary JSON files used to store the results of each subprocess are removed to clean up the storage.\n",
    "The total execution time for the entire multiprocessing operation is calculated and displayed.\n",
    "The hyperparameters of the best model and its performance metrics (RMSE and MAPE) are printed out as the final output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by importing the necessary libraries\n",
    "import multiprocessing # For parallel processing\n",
    "import json # For saving the best parameters to a file\n",
    "import os # For checking if the file exists and deleting it after the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the evaluation in a function and save the results to a json file\n",
    "def evaluate_model(n_estimators, max_features, max_depth, file_path):\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_features=max_features,\n",
    "        max_depth=max_depth,\n",
    "        random_state=42\n",
    "    )\n",
    "    rf_model.fit(X_train_filled, y_train)\n",
    "    y_val_pred = rf_model.predict(X_val_filled)\n",
    "    rmse = sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "    mape = mean_absolute_percentage_error(y_val, y_val_pred) * 100\n",
    "    \n",
    "    # Save results to a JSON file\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'n_estimators': n_estimators,\n",
    "            'max_features': max_features,\n",
    "            'max_depth': max_depth,\n",
    "            'rmse': rmse,\n",
    "            'mape': mape\n",
    "        }, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time() # Start the timer\n",
    "processes = [] # Initialize a list to store the processes\n",
    "temp_files = [] # Initialize a list to store the file paths\n",
    "\n",
    "for n_estimators in n_estimators_range: # Loop over the parameter ranges\n",
    "    for max_features in max_features_range:\n",
    "        for max_depth in max_depth_range:\n",
    "            file_path = f'results/temp_results_{n_estimators}_{max_features}_{max_depth}.json'\n",
    "            temp_files.append(file_path)\n",
    "            process = multiprocessing.Process(target=evaluate_model, args=(n_estimators, max_features, max_depth, file_path)) # Create a process for each combination of parameters\n",
    "            processes.append(process) # Add the process to the list\n",
    "            process.start()\n",
    "\n",
    "# Wait for all processes to finish\n",
    "for process in processes:\n",
    "    process.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The multiprocessing execution time is 17.05547070503235\n",
      "The best parameters {'n_estimators': 100, 'max_features': None, 'max_depth': None} for RMSE = 26057.941851126383, MAPE: 9.868196740754167%\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables to store the best RMSE and its parameters\n",
    "best_rmse = float('inf')\n",
    "best_mape = float('inf')\n",
    "best_parameters = {}\n",
    "\n",
    "# Collect results from JSON files\n",
    "for file_path in temp_files: # Loop over the temporary files\n",
    "    with open(file_path, 'r') as f: \n",
    "        results = json.load(f) # Load the results from the file\n",
    "        if results['rmse'] < best_rmse: # If the results are better than the current best, update the best RMSE and its parameters\n",
    "            best_rmse = results['rmse']\n",
    "            best_mape = results['mape']\n",
    "            best_parameters = {\n",
    "                'n_estimators': results['n_estimators'],\n",
    "                'max_features': results['max_features'],\n",
    "                'max_depth': results['max_depth']\n",
    "            }\n",
    "    os.remove(file_path)  # Clean up the temporary file\n",
    "\n",
    "end_time = time.time() # End the timer\n",
    "multiprocessing_time = end_time - start_time\n",
    "print(f\"The multiprocessing execution time is {multiprocessing_time}\")\n",
    "print(f\"The best parameters {best_parameters} for RMSE = {best_rmse}, MAPE: {best_mape}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sequential Execution Time: 59.58 seconds\n",
    "- Threaded Execution Time: 35.16 seconds\n",
    "- Multiprocessing Execution Time: 17.06 seconds\n",
    "\n",
    "Execution Time Changes:\n",
    "\n",
    "The threaded implementation is faster than the sequential one, likely due to parallel execution of some tasks.\n",
    "The multiprocessing implementation is the fastest, significantly improving performance by effectively using multiple CPU cores and avoiding the limitations of Python's Global Interpreter Lock (GIL).\n",
    "Performance Metrics:\n",
    "\n",
    "Ideally, all three methods (sequential, threaded, multiprocessing) should give you the same best model accuracy (RMSE, MAPE), meaning they all find the same best set of hyperparameters.\n",
    "\n",
    "\n",
    "The main benefit of threading and multiprocessing is reducing the time it takes to find the best model, not necessarily improving the model's accuracy itself.\n",
    "\n",
    "\n",
    "In summary, multiprocessing offered the best speed improvement in your hyperparameter tuning task, likely due to its ability to better utilize the CPU resources for this particular computation-heavy task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "featureengineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
